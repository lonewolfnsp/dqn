{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Frozen Lake with Q-learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. \n",
    "\n",
    "At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "SFFF       \n",
    "FHFH       \n",
    "FFFH       \n",
    "HFFG\n",
    "\n",
    "S: starting point, safe  \n",
    "F: frozen surface, safe  \n",
    "H: hole, fall to your doom  \n",
    "G: goal, where the frisbee is located\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole.  \n",
    "You receive a reward of 1 if you reach the goal, and 0 otherwise.\n",
    "\n",
    "https://gym.openai.com/envs/FrozenLake-v0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "print(f'q_table gas shape: {q_table.shape} ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "lr = 0.05\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "exploration_decay_rate = 0.00005"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_Q_value( newState, reward):\n",
    "    return reward + discount_rate*np.max(q_table[newState,:])\n",
    "\n",
    "def render():\n",
    "    clear_output(wait=True)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newState, reward, done, _, _=env.step(0)\n",
    "newState"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curses import curs_set\n",
    "from socket import getfqdn\n",
    "\n",
    "\n",
    "ep_rewards=[]\n",
    "ep1000_rewards=[]\n",
    "for ep in range(num_episodes):\n",
    "    rewards=0. \n",
    "    exploration_rate=1. \n",
    "    curState=env.reset()\n",
    "    curState=curState[0]\n",
    "    action=-1\n",
    "    for step in range(max_steps_per_episode):\n",
    "        exploration_rate-=(exploration_decay_rate*step*ep )\n",
    "        dice=np.random.uniform(0, 1)\n",
    "        if dice < exploration_rate:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action=np.argmax(q_table[curState, :])\n",
    "        \n",
    "        newState, reward, done, _,_ = env.step(action)\n",
    "        newQVal=compute_new_Q_value( newState, reward)\n",
    "        q_table[curState, action]=(1-lr)*q_table[curState, action] + (lr*newQVal        )\n",
    "        rewards+=reward \n",
    "        curState=newState\n",
    "        if done or step==max_steps_per_episode:\n",
    "            ep_rewards.append(rewards)\n",
    "            if (ep+1) % 1000 == 0:\n",
    "                ep1000_rewards.append(np.sum(ep_rewards))\n",
    "                print(f'ep {ep+1}: {ep1000_rewards[-1] } ')\n",
    "                ep_rewards.clear()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(ep1000_rewards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the game plays out\n",
    "### Now we run through a game with actions defined by the Q table we trained. It should find the optimal path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curState=env.reset()\n",
    "curState=curState[0]\n",
    "for step in range(max_steps_per_episode):\n",
    "    render()\n",
    "    time.sleep(1)\n",
    "    # Get best action from q-table for current state \n",
    "    action=np.argmax(q_table[curState, : ])\n",
    "    newState, reward, done, _, _ = env.step(action)\n",
    "    curState=newState\n",
    "    if done or (step+1)==max_steps_per_episode:\n",
    "        render()        \n",
    "        if reward==1:\n",
    "            print(f'\\n you won after {step} steps')\n",
    "        else:\n",
    "            print(f'you lost after {step} steps')\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the env after playing\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
