{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar lander solved with DQN using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils \n",
    "from replaybuffer import ReplayBuffer\n",
    "import torch \n",
    "import gymnasium as gym\n",
    "import numpy as np \n",
    "import copy \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters And constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes= 5000\n",
    "max_steps_in_episode=500\n",
    "learning_rate= 1e-3\n",
    "gamma=0.995\n",
    "max_replays=100000\n",
    "batch_size=64\n",
    "train_interval=3\n",
    "tau= 1e-3 # used for soft update of target network after training. takes a fraction of the model's trained weights\n",
    "\n",
    "layers=[64,64]\n",
    "MSE = torch.nn.MSELoss()\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "memories=ReplayBuffer(max_replays, batch_size, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_actions=4\n",
      " state_size=(8,) \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "Actions=np.arange(num_actions)\n",
    "print(f'num_actions={num_actions}\\n state_size={state_size} ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instantiating the deep Q networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=utils.DQN(state_size[0],  num_actions, layers)\n",
    "target=copy.deepcopy(model) # target is a clone of the Q network \n",
    "opt=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "target.eval() # target is not trained. Uses soft update to update its weights instead\n",
    "model.train()\n",
    "target.to(device)\n",
    "model.to(device)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs exploitation\n",
    "### When training starts, exploration is favoured over exploitation. $\\epsilon$ decays over training session to favour exploitation over exploration as model learns to take the right steps for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1.0 # starting epsilon\n",
    "decay=0.995 # decay factor per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(dqnModel, epsilon, curState):\n",
    "    model.eval()\n",
    "    action=-1    \n",
    "    if np.random.uniform(0,1) < max(0.05, epsilon) :\n",
    "        action=np.random.choice(Actions) # randomly pick an action       \n",
    "    else:\n",
    "        action=utils.getQAction(dqnModel, state,device)\n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Q network\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y_j =\n",
    "    \\begin{cases}\n",
    "      R_j & \\text{if episode terminates at step  } j+1\\\\\n",
    "      R_j + \\gamma \\max_{a'}\\hat{Q}(s_{j+1},a') & \\text{otherwise}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDQN():\n",
    "    model.train()\n",
    "    states, actions, rewards, next_states, dones=memories.sample()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_qsa=target(next_states)\n",
    "\n",
    "    max_qsa, _=torch.max(max_qsa, dim=1)\n",
    "    y_targets=rewards + gamma* max_qsa * (1. - dones)\n",
    "\n",
    "    qsa=model(states)    \n",
    "    vals=qsa.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    opt.zero_grad()\n",
    "    loss=MSE(y_targets, vals)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # soft update the target network \n",
    "    utils.softupdate(target, model, tau)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start collecting experiences and train the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode:100 \t Average rewards= -164.97 \n",
      " Episode:200 \t Average rewards= -88.35 \n",
      " Episode:300 \t Average rewards= 15.39 \n",
      " Episode:400 \t Average rewards= 41.23 \n",
      " Episode:500 \t Average rewards= 61.48 \n",
      " Episode:600 \t Average rewards= 80.89 \n",
      " Episode:700 \t Average rewards= 103.69 \n",
      " Episode:800 \t Average rewards= 230.97 \n",
      " Episode:900 \t Average rewards= 232.61 \n",
      " Episode:1000 \t Average rewards= 239.30 \n",
      " Episode:1100 \t Average rewards= 245.12 \n",
      " Episode:1200 \t Average rewards= 224.27 \n",
      " Episode:1300 \t Average rewards= 220.46 \n",
      " Episode:1400 \t Average rewards= 204.07 \n",
      " Episode:1500 \t Average rewards= 202.41 \n",
      " Episode:1600 \t Average rewards= 193.01 \n",
      " Episode:1700 \t Average rewards= 233.54 \n",
      " Episode:1800 \t Average rewards= 252.49 \n",
      " Episode:1900 \t Average rewards= 257.55 \n",
      " Episode:2000 \t Average rewards= 263.96 \n",
      "\n",
      "\n",
      " training ends early \n"
     ]
    }
   ],
   "source": [
    "AverageRewards=[]\n",
    "Rewards=0.\n",
    "showRes=100\n",
    "totalRewards=0. \n",
    "cnt=0\n",
    "\n",
    "for ep in range (1, (max_episodes+1),1):\n",
    "    state=env.reset()\n",
    "    state=state[0]\n",
    "    Rewards=0.\n",
    "    epsilon*=decay\n",
    "    for step in range(1, (max_steps_in_episode+1), 1):        \n",
    "        cnt+=1\n",
    "        action=getAction(model, epsilon, state)\n",
    "        next, reward, done, _,_ = env.step(action)        \n",
    "        memories.add(state, action, reward,next, done)\n",
    "        Rewards+=reward\n",
    "        if len(memories)> batch_size and cnt % train_interval==0:            \n",
    "            trainDQN()\n",
    "        state=next \n",
    "        if done or step ==max_steps_in_episode:\n",
    "            break        \n",
    "    print(f\"\\r episode: {ep} \\t reward: {Rewards:.2f}\", end=\"\")\n",
    "    totalRewards+=Rewards\n",
    "    if ep % showRes==0:                  \n",
    "        AverageRewards.append(totalRewards/showRes)     \n",
    "        print(f'\\r Episode:{ep} \\t Average rewards= {AverageRewards[-1]:.2f} ') \n",
    "        totalRewards=0.  \n",
    "        if AverageRewards[-1] > 260:\n",
    "            print('\\n\\n training ends early ')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhF0lEQVR4nO3deXzV1Z3/8dfJTlaWLIRASJCwBBDBCCpLRZ0RBfdpxdq6S9ufztSZ6VQ7ba39TW11OnVqW2vFVq1tLdqxjgvgvhBEQEAIexKSACEhC0tWEpLcM3/cC40xgUDuvd+7vJ+PRx6593y/l/vx5Pq+33u+53uusdYiIiLhJcLpAkRExP8U/iIiYUjhLyIShhT+IiJhSOEvIhKGopwuoL9SU1NtTk6O02WIiASVDRs21Ftr03q2B0345+TksH79eqfLEBEJKsaYPb21a9hHRCQMKfxFRMKQwl9EJAwp/EVEwpDCX0QkDCn8RUTCkMJfRCQMBc08fxGRcNHe2UVJTTPbqxupqG/h2/MneP05FP4iIg6qa2pnR3Vjt58mdtc10+lyf9fKoOhIFs8dw+D4GK8+r8JfRMQPOrtclNW3sKO6ke2ekN9R3UhdU/uJfTJT4piYmcyl+enkZ6YwMTOJ0cMSiIwwXq9H4S8i4kXtnV3sO9RKWV0LFQdbKK1tZkd1E7tqmjjW6QIgOtKQl57E3Lw0JmYmkT8imYnDkxmS4N2j+5NR+IuInKbOLheVh49SXt9Ceb075I/frjpyFFe3b8cdlhDDxMxkbrlgNBMzk8kfkcxZaYlERzo730bhLyLSh7aOLtZXHKa8vpny+tYTIb/vUOuJMXmApNgoctMSmJ49hOumj2RMagI5qQnkDksgJT7awf+Cvin8RUR66HJZ/rqxkkffLqa6oQ1wn3jNSU1gYmYSl08eTm5qArmekB+WEIMx3h+X9yWFv4iIh7WWD4rreGTFTnYeaGLqyBT+4+rJTM5KISM5NugC/mQU/iIiwJbKBn6yYgerdx9k9LB4Hv/ydK6YMjykAr87hb+IFzW0dlBc20RxTRMlNc2U1DYxeFAM37psPLmpCU6XJ73Yd6iVn765i1c3VzE0IYYfXjWJG2dkExMV2gsgKPxFzkDD0Q5Kapoorml2B32tO+xru83Zjo+JJC89kaJ9Dby9vYbFc8dw97yxDIqJdLByOe5wyzF++V4pf1hTQWSE4Z55Y/naF8aQFBeYJ2i9TeEvchJNbR0Udw95z9F8TePfQn5QdCR5GYnMyUtjXEYi4zKSyMtIZETKICIiDLVNbTy8fCe/er+Ulz/dz/cX5nPZpIyQHU4IdG0dXTz9UTlPvL+blmOd3HDeKO69dBwZyXFOl+ZXxlp76r0CQEFBgdV3+Iov1Te3s62qkW1VDWzb7/5dcbD1xPZB0ZGMTU8kzxPw4zISyUtPImuwO+RPZV35IR54ZSs7DzQxd1waP7xqkoaC/KjLZXlpYyWPvlXMgcY2Lp2Yzn3zJ5CXkeR0aT5ljNlgrS34XLvCX8KNtZb9R466g35/gyfwGznQ2HZin1FDBzEpM4XJWclMzExmXEb/Q/5kOrtcPPfxHv777WLaO13cNTeXu+eNJT5GH8J9xVrLB7vqeHjFTnbVNDF11GD+/fIJzBwzzOnS/ELhL2HHWkt7p/tKzG1VDX87qq9q5EhrBwARBs5KS2RyVgqTRrivvpyUmeLzC3OODwX99dP9ZA0exPcXTuSySaE7s8QpZXXNfPflrXxcdpCcYfF8e/4ELp8cXv2s8JegVNvYxoY9h2k51kVLeyctxzppbe/67G/PttZj3do9+3a/zD4mMoLxw5OYnJVM/gh32E8cnuzoCdieQ0EPXpnPmLREx+oJJbVNbVz7+GpajnXyL383jhtnZDu+pIITFP4SlL7y27WsKq3/TFuEgYSYKOJjI0mIiSIhNor4mMi//e62LT42krTEWCZnpTA23fn1VHqjoSDvO3qsi0VLPqa4ppm/fP0CJmelOF2SY/oKf726JGC1HutkbflBbpwxiv930dgTAR8bFRFSH9ujIiO4fXYuC6dm8vDynTz+/m5e3rifB67M11DQGXC5LPe+8ClF+xtY8tWCsA7+kwm8wyARj7Vlh+joslwxJZNRQ+MZlhhLXHRkyIZhelIcj95wDi9+7QKSB0Xz9T9u5Oan17Fx72EOtxwjWD6lO+3hN3by5rYavr8gn7/Lz3C6nIClI38JWCtL6oiNiuC8nKFOl+JXM3KH8vo/zuYPa/bw6FvFXPfr1QDERkWQmRLH8JQ4RqQMYnhKnOf+oBPtwbjAmDf9ae0elqws4+YLRnPbrBynywloCn8JWIUl9czIHUpcdPhdERsVGcFts3K5cuoI1pYd4kBjGwcajlLd0EZ1Qxtryw9R09j2mWWFwX1Se7jnjeD4G0J+ZjJXTMkMyPMd3vRhcR0PvLKNeePTeGBhfli/CfaHwl8CUtWRo5TWNnNDwSinS3FUamIsC87O7HVbl8tysLn9xBvCgYajVDe2ccBz/9O9RzjQ0MaxLhePrNjJXXPHcMN5o0LyRPKuA03c/aeNjMtI4pdfnk5UiL/ReUPovQokJKwqcc/wmTMu1eFKAldkhCE9OY705Dim9vEe6XJZPiyp44n3d/PD17bzy/dKufXCHG65ICdgv2TkdNU2tnH7s5+QEBvJ07cWkBirWOsP9ZIEpMLSetKSYhkf4pfe+1pEhGHe+HTmjU9nfcUhnvhgN4++XcyTH+7mpvNHc8fs3KBe06b1WCd3Preew63HePFrF5CZMsjpkoKGwl8CjstlWVVSx7wJ6Rq39aKCnKH87tah7Khu5Dcf7ua3hWU8+1EF15+bxdfmnkVOkK0z1OWy/PMLm9iqKZ1nRANjEnC2VTVyuLWDuXlpTpcSkiZmJvPYoml88K15fOm8kby0cT8X/+wD7n5+I1v3NzhdXr89vGKHe0rnwnwu1ZTO06Yjfwk4K0vqAJg1VuP9vpQ9LJ4fXTOFf7okj6dXVfDHNXtYVlTNF8al8Y2LzmJm7tCA/eT1xzV7eKqwnFsuGM1ts3KdLico6chfAk5hSR35mcmkJcU6XUpYSE+K4/7LJ/DR/Rfzb5eNZ+v+BhYtWcP1T6zmne01uFyBdXHZh8V1/ODVbVw8IZ3vL8x3upygNeAjf2PMKOA5YDjgApZYax8zxgwFXgBygArgS9baw57HfAe4A+gC/sla++ZA65DQ0NLeyYY9h7ldR3N+lzIomrvnjeWO2bm8uH4fT35Yxp3PrWdoQgyDB0V/bg2lxNgo4mOiSIiNPPE7oef92CjSk+IYmhDjlRp3Hmg8MaXzFzdO05TOAfDGsE8n8K/W2o3GmCRggzHmbeBW4F1r7cPGmPuB+4H7jDH5wCJgEjACeMcYM85a2+WFWiTIrS0/SEeXZY7G+x0TFx3JzRfkcOOMbF4vqmLN7kO0HOv0rJTaRW1TG63tXTR3W0n1VCtPTBiexNxxacwem3rGF+7VNrZx+zOa0uktA+49a201UO253WSM2QFkAVcDF3l2+z3wAXCfp32ptbYdKDfGlAIzgI8HWosEv5XF9cRGRVCQM8TpUsJedGQE104bybXTRp50P2stbR2uv71BtHfReqzzxDLc5fUtrCqp59mPKliysoyYqAjOyxnCnDz3m0F+ZvIpvyTn+JTOI0c7NKXTS7z61mmMyQGmAWuBDM8bA9baamNMume3LGBNt4dVetp6+/cWA4sBsrOzvVmqBKjCkjpmjhkWlks6BCtjDINiIhkUE0lqYu/nae6eN5bWY52sKz9EYUk9q0rqeXjFTgCGJcRw4dhU5oxNZXZeKiMGfzbYu1yWe5e6p3Q+dbOmdHqL18LfGJMIvATca61tPMksgd429Pqh0Vq7BFgC7vX8vVGnBK79R46yu66FG2fojT4UxcdEcdH4dC4a7z4OrG1sY1Wp+42gsLSe1zZXAXBWWsKJTwXnnzWMx94p5q3tNTx4ZT6XTNSUTm/xSvgbY6JxB/+frLV/9TTXGGMyPUf9mUCtp70S6H4x+kigyht1SHBb5ZniqfH+8JCeHMd100dy3fSRWGvZVdPkfiMoqWfpJ3t5dnUFURGGTpfl1gtzuFWTALzKG7N9DPA7YIe19tFum14FbgEe9vx+pVv788aYR3Gf8M0D1g20Dgl+K0vqSU+KZVyGvsYw3BhjmDA8mQnDk7lzzhjaO7vYUHGYwtJ6XC7Lt+dPcLrEkOONI/9ZwFeBLcaYTZ62f8cd+i8aY+4A9gJfBLDWbjPGvAhsxz1T6G7N9JEul+Wj0noumZARsBcWif/ERkVy4dhULtSFfj7jjdk+q+h9HB/gkj4e8xDw0ECfW0LHtqoGjrR2MFereIr4ha6QkIBQ6FnCWUs6iPiHwl8CwsriOiaNSO5zqqCIeJfCXxzX3N7Jxr2HmZ2no34Rf1H4i+PWlrmXdNASziL+o/AXxxWW1BMXHcG5o7Wkg4i/KPzFcStL6piZqyUdRPxJ4S+OqjzcSlldC3M03i/iVwp/cdQqzxTPueM03i/iTwp/cVRhST0ZybHkpWtJBxF/UviLY7pcllWl9czJS9OSDiJ+pvAXx2zd30DD0Q6N94s4QOEvjin0LOE8W0s6iPidwl8cs7KknslZyQzTkg4ifqfwF0c0t3eycc9hfXGLiEMU/uKINbsP0umyzNGQj4gjFP7iiMKSOveSDjla0kHECQp/cURhST3njxlGbJSWdBBxgsJf/G7foVbK6ls03i/iIIW/+N2qUs+SDprfL+IYhb/4XWFJHcOT4xirJR1EHKPwF7/qcllWldQzJy9VSzqIOEjhL35VVHmExrZO5mgVTxFHKfzFr1aV1GOMlnQQcZrCX/yqsKSeySNSGJoQ43QpImFN4S9+09TWwca9h7WKp0gAUPiL36wpO+Re0kHz+0Ucp/AXvyksqWNQdCTTRw92uhSRsKfwF79xL+kwVEs6iAQAhb/4xb5DrZRrSQeRgKHwF78oLPEs6TBOJ3tFAoHCX/yisKSOzJQ4zkrTkg4igUDhLz7X2eXio1It6SASSBT+4nNF+xvcSzpovF8kYCj8xecKi91LOszSkg4iAUPhLz5XWFLHlCwt6SASSBT+4lNNbR18uu+IlnQQCTAKf/Gpj3cfpEtLOogEHK+EvzHmaWNMrTFma7e2ocaYt40xJZ7fQ7pt+44xptQYs8sYc5k3apDA0+WyvLK5iviYSKZnDzn1A0TEb7x15P8sML9H2/3Au9baPOBdz32MMfnAImCS5zG/Nsboev8QU1LTxPVPrGZZUTVfKhhFTJQ+ZIoEkihv/CPW2pXGmJwezVcDF3lu/x74ALjP077UWtsOlBtjSoEZwMfeqEWc1dHl4skPd/OLd0tJiI3ksUXncNXUEU6XJSI9eCX8+5Bhra0GsNZWG2PSPe1ZwJpu+1V62j7HGLMYWAyQnZ3tw1LFG7ZVNfBvfylie3UjC87O5IdXTSI1MdbpskSkF74M/770domn7W1Ha+0SYAlAQUFBr/uI89o7u/jVe6U88cFuBsfH8JuvnMv8ycOdLktETsKX4V9jjMn0HPVnArWe9kpgVLf9RgJVPqxDfOjTvYf59v8UUVLbzPXTR/L9hRMZHK/5/CKBzpdn4V4FbvHcvgV4pVv7ImNMrDEmF8gD1vmwDvGBo8e6+PHyHVz/xGqa2zt55rbz+NmXpir4RYKEV478jTF/xn1yN9UYUwn8AHgYeNEYcwewF/gigLV2mzHmRWA70Ancba3t8kYd4h9ryw5y30tFVBxs5cszs/nO5RNIiot2uiwROQ3emu1zYx+bLulj/4eAh7zx3OI/Le2dPPLGTp77eA/ZQ+N5/q6ZXHiWrtwVCUZOnPCVILSqpJ77XiqiquEot8/K5VuXjSM+Ri8fkWCl/3vlpBqOdvCT5TtY+sk+xqQl8D9fv4BzRw91uiwRGSCFv/Sqs8vF0k/28d9vF3PkaAffuOgsvnlJHnHRuhhbJBQo/OUzrLV8sKuOHy/fQUltMzNyh/LAwnwmZ6U4XZqIeJHCX07YXtXIj5fvYFVpPbmpCTz51XP5+/wMffWiSAhS+As1jW387K1d/GVDJSmDovnBlfncNHO0FmMTCWEK/zDWeqyTJSvLePLDMjpdLu6cncs98/JIidecfZFQp/APQ10uy0sbK/nZW7uoaWxnwZRM7ps/gexh8U6XJiJ+ovAPM6tK6nlo+Q52VDcyLXswv75puqZuioQhhX+YKK1t4sfLd/LezlpGDhnEL2+cxsKzM3UyVyRMKfxDXH1zOz9/p5g/r9tHfHQk37l8ArdcmKP5+iJhTuEfojq6XPx+dQWPvVNCa0cXN83M5puX5DFMX64iIij8Q9KHxXX8/9e2sbuuhS+MS+P7C/MZm57odFkiEkAU/iGkor6FHy3bzjs7askZFs/vbing4gnpGtcXkc9R+IeA5vZOfvVeKU+vKic60nD/5RO4bVYOsVEa1xeR3in8g5jLZXn50/088sZOapvauW56FvfPn0B6cpzTpYlIgFP4B6nN+47w4Gvb+HTvEaaOGsyTXz2XadlDnC5LRIKEwj/I1Da18dM33OvwpCbG8l9fnMp107KIiNC4voj0n8I/SBzrdPHs6nJ+8W4p7Z1dfG3uGO65eKy+O1dEzojCPwi8v7OW/3h9O2X1LVw8IZ3vLZjImDRN3RSRM6fwD2BHWo9x30tFvLmthjGpCTxz23nMG5/udFkiEgIU/gFqw55D/OPzn1LX3M598ydwx+xcra8vIl6j8A8wLpflyZVl/Ndbu8gaPIiXvnEhZ48c7HRZIhJiFP4B5GBzO//y4mY+LK5jwZRMfnL9FJJ1QldEfEDhHyDWlB3km0s/5XBrBz+6ZjI3zczWsgwi4jMKf4d1uSy/eq+Ux94tJmdYAs/cOoP8EclOlyUiIU7h76DapjbuXbqJ1bsPcs05I/jRtVNIjNWfRER8T0njkMKSOv75hU00t3fyn/9wNl88d6SGeUTEbxT+ftbZ5eLn75Tw+AeljE1L5Pm7zmdcRpLTZYlImFH4+1F1w1G++edNrKs4xA0Fo3jwqkkMitGyyyLifwp/P3lvZw3/+uJm2jtd/PyGc7hmWpbTJYlIGFP4+1hHl4ufvrmLJSvLmJiZzONfnqZ1eUTEcQp/H7t36SaWbanmK+dn870F+cRFa5hHRJyn8Peh+uZ2Vmyt5s7ZuXxvYb7T5YiInKCVwnzoja0HcFn4h4KRTpciIvIZCn8fWlZUzZi0BMZrKqeIBBiFv4/UNbWztvwgC6dk6uItEQk4joW/MWa+MWaXMabUGHO/U3X4yhvb3EM+C84e4XQpIiKf40j4G2MigceBy4F84EZjTEidEV1WVMXY9ETGZWhap4gEHqeO/GcApdbaMmvtMWApcLVDtXhdbVMba8sPsUBDPiISoJwK/yxgX7f7lZ62zzDGLDbGrDfGrK+rq/NbcQP1xtYDWAsLzs50uhQRkV45Ff69HQ7bzzVYu8RaW2CtLUhLS/NDWd6xrKiavPRELdgmIgHLqfCvBEZ1uz8SqHKoFq+qbWxjXcUhHfWLSEBzKvw/AfKMMbnGmBhgEfCqQ7V41YrjQz5TFP4iErgcWd7BWttpjLkHeBOIBJ621m5zohZvW1ZUzfiMJPI05CMiAcyxtX2stcuB5U49vy/UNLbxyZ5D3HvJOKdLERE5KV3h60UrtlR7ZvkMd7oUEZGTUvh70bIt1UwYnsTYdA35iEhgU/h7yYGGNj6pOKwTvSISFBT+XrJ8SzUAV2iKp4gEAYW/lxwf8jlLX9EoIkFA4e8F1Q1H2bDnMAt11C8iQULh7wXLtxwA4AqN94tIkFD4e8GyoiryM5MZoyEfEQkSCv8B2n/kKBv3HtFaPiISVBT+A7Ti+CwfDfmISBBR+A/Qsi3VTBqRTG5qgtOliIj0m8J/ACoPt/KphnxEJAgp/AdghWeWj67qFZFgo/AfgNe3VDM5K5nRwzTkIyLBReF/hvYdamXzviMsmDLC6VJERE6bwv8MrdjqnuWjIR8RCUYK/zO0rKias0emkD0s3ulSREROm8L/DOw71MrmygbN7ReRoKXwPwPLtmjIR0SCm8L/DCzfUs3UkSmMGqohHxEJTgr/07T3YCtFlQ26sEtEgprC/zQt01o+IhICFP6nadmWKqaOGszIIRryEZHgpfA/DXsOtrB1fyMLddQvIkFO4X8ajg/5XD5luMOViIgMjML/NCwrqmZatoZ8RCT4Kfz7qby+hW1VjZrbLyIhQeHfT8s1y0dEQojCv59eL6pmevZgRgwe5HQpIiIDpvDvh7K6ZnZUN7LgbC3fLCKhQeHfD38b8tEsHxEJDQr/fni9qJqC0UPITNGQj4iEBoX/KZTWNrPzQJNO9IpISFH4n4Jm+YhIKFL4n4TLZfnfTfspGD2E4SlxTpcjIuI1Cv+TeH9XLWV1LXzl/NFOlyIi4lUK/5NYsrKMESlxWrtfRELOgMLfGPNFY8w2Y4zLGFPQY9t3jDGlxphdxpjLurWfa4zZ4tn2C2OMGUgNvrJ53xHWlh/i9tm5REfqPVJEQstAU20rcB2wsnujMSYfWARMAuYDvzbGRHo2PwEsBvI8P/MHWINPPFVYRlJsFDecN8rpUkREvG5A4W+t3WGt3dXLpquBpdbadmttOVAKzDDGZALJ1tqPrbUWeA64ZiA1+MK+Q60s31LNl2dmkxQX7XQ5IiJe56vxjCxgX7f7lZ62LM/tnu29MsYsNsasN8asr6ur80mhvXn6o3IijOHWWTl+e04REX+KOtUOxph3gN7WNfiutfaVvh7WS5s9SXuvrLVLgCUABQUFfe7nTQ2tHbzwyT6umjpCV/SKSMg6Zfhbay89g3+3Eug+WD4SqPK0j+ylPWD8ce0eWo91ceecMU6XIiLiM74a9nkVWGSMiTXG5OI+sbvOWlsNNBljzvfM8rkZ6OvTg9+1d3bx7OoK5uSlkj8i2elyRER8ZqBTPa81xlQCFwDLjDFvAlhrtwEvAtuBN4C7rbVdnod9A/gt7pPAu4EVA6nBm17ZVEVdUzuL5+qoX0RC2ymHfU7GWvsy8HIf2x4CHuqlfT0weSDP6wvWWp5aWcaE4UnMHpvqdDkiIj6lq5c8Piiuo6S2mcVzxxCg152JiHiNwt/jqZVlDE+OY6G+rUtEwoDCH9i6v4HVuw9y26wcYqLUJSIS+pR0uJdySIyN4saZ2U6XIiLiF2Ef/pWHW3m9qJpF540iWUs5iEiYCPvwf+ajCgBum53rbCEiIn4U1uHfcLSDpev2cuXZmWQN1lIOIhI+wjr8/7xuLy1aykFEwlDYhv+xThfPfFTOrLHDmJyV4nQ5IiJ+Fbbh/9rmKmoa27lLR/0iEobCMvyttTxVWMb4jCS+MC7N6XJERPwuLMO/sKSenQeauHNOrpZyEJGwFJbhv2RlGelJsVx1jpZyEJHwFHbhv62qgVWl9dw6K4fYqMhTP0BEJASFXfj/trCchJhIbpo52ulSREQcE1bhX3XkKK9truKG87JJGaSlHEQkfIVV+D+7ugIL3DYrx+lSREQcFTbh39jWwfNr93LFlExGDY13uhwREUeFTfi/sG4fze2d3DVHC7iJiIRF+Hd0uXj6o3LOHzOUs0cOdrocERHHhUX4v15URXVDG4vnaikHEREIg/C31rJkZTlj0xO5aFy60+WIiASEkA//j0oPsqO6kcVzxhARoaUcREQgDMJ/SWEZqYmxXD1NSzmIiBwX5XQBvuRyWcZnJHLRuDQt5SAi0k1Ih39EhOG7C/KdLkNEJOCE/LCPiIh8nsJfRCQMKfxFRMKQwl9EJAwp/EVEwpDCX0QkDCn8RUTCkMJfRCQMGWut0zX0izGmDthzhg9PBeq9WI63qb6BUX0Do/oGJtDrG22tTevZGDThPxDGmPXW2gKn6+iL6hsY1Tcwqm9gAr2+vmjYR0QkDCn8RUTCULiE/xKnCzgF1Tcwqm9gVN/ABHp9vQqLMX8REfmscDnyFxGRbhT+IiJhKKTC3xgz3xizyxhTaoy5v5ftxhjzC8/2ImPMdD/WNsoY874xZocxZpsx5pu97HORMabBGLPJ8/OAv+rzPH+FMWaL57nX97Ldyf4b361fNhljGo0x9/bYx6/9Z4x52hhTa4zZ2q1tqDHmbWNMief3kD4ee9LXqg/r+6kxZqfn7/eyMWZwH4896WvBh/U9aIzZ3+1veEUfj3Wq/17oVluFMWZTH4/1ef8NmLU2JH6ASGA3MAaIATYD+T32uQJYARjgfGCtH+vLBKZ7bicBxb3UdxHwuoN9WAGknmS7Y/3Xy9/6AO6LVxzrP2AuMB3Y2q3tP4H7PbfvBx7po/6TvlZ9WN/fA1Ge24/0Vl9/Xgs+rO9B4Fv9+Ps70n89tv8MeMCp/hvoTygd+c8ASq21ZdbaY8BS4Ooe+1wNPGfd1gCDjTGZ/ijOWlttrd3oud0E7ACy/PHcXuRY//VwCbDbWnumV3x7hbV2JXCoR/PVwO89t38PXNPLQ/vzWvVJfdbat6y1nZ67a4CR3n7e/uqj//rDsf47zhhjgC8Bf/b28/pLKIV/FrCv2/1KPh+u/dnH54wxOcA0YG0vmy8wxmw2xqwwxkzyb2VY4C1jzAZjzOJetgdE/wGL6Pt/Oif7DyDDWlsN7jd8IL2XfQKlH2/H/UmuN6d6LfjSPZ5hqaf7GDYLhP6bA9RYa0v62O5k//VLKIW/6aWt5zzW/uzjU8aYROAl4F5rbWOPzRtxD2VMBX4J/K8/awNmWWunA5cDdxtj5vbYHgj9FwNcBfyll81O919/BUI/fhfoBP7Uxy6nei34yhPAWcA5QDXuoZWeHO8/4EZOftTvVP/1WyiFfyUwqtv9kUDVGezjM8aYaNzB/ydr7V97brfWNlprmz23lwPRxphUf9Vnra3y/K4FXsb98bo7R/vP43Jgo7W2pucGp/vPo+b4UJjnd20v+zj9OrwFWAjcZD0D1D3147XgE9baGmttl7XWBTzVx/M63X9RwHXAC33t41T/nY5QCv9PgDxjTK7n6HAR8GqPfV4FbvbMWjkfaDj+Ed3XPGOEvwN2WGsf7WOf4Z79MMbMwP33Oein+hKMMUnHb+M+Mbi1x26O9V83fR5xOdl/3bwK3OK5fQvwSi/79Oe16hPGmPnAfcBV1trWPvbpz2vBV/V1P4d0bR/P61j/eVwK7LTWVva20cn+Oy1On3H25g/u2SjFuGcCfNfT9nXg657bBnjcs30LUODH2mbj/mhaBGzy/FzRo757gG24Zy+sAS70Y31jPM+72VNDQPWf5/njcYd5Src2x/oP95tQNdCB+2j0DmAY8C5Q4vk91LPvCGD5yV6rfqqvFPd4+fHX4G961tfXa8FP9f3B89oqwh3omYHUf572Z4+/5rrt6/f+G+iPlncQEQlDoTTsIyIi/aTwFxEJQwp/EZEwpPAXEQlDCn8RkTCk8BcRCUMKfxGRMPR/47JFfGwabwQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(AverageRewards)\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode='human' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rewards gaained: 178.43"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5640/127349611.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/john/data1/git/dqn2/utils.py\u001b[0m in \u001b[0;36mrunDQNAgent\u001b[0;34m(dqnModel, env, device, max_step, fps)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\r rewards gaained: {rewards:.2f}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mstep\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbreakout\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "utils.runDQNAgent(model, env, device, fps=25)\n",
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.saveTrainedModel(model, 'lunarlander')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load trained model to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainedModel= utils.DQN(state_size[0],  num_actions, layers)\n",
    "utils.loadModel(trainedModel, \"weights/lunarlander\")\n",
    "print(trainedModel)\n",
    "trainedModel.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  run the trained agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode='human' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rewards gaained: 290.97\n",
      "Terminated with rewards=290.97\n"
     ]
    }
   ],
   "source": [
    "utils.runDQNAgent(trainedModel, env, device, fps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
