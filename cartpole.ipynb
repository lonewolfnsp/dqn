{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole solved with DQN using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils \n",
    "from replaybuffer import ReplayBuffer\n",
    "import torch \n",
    "import gym\n",
    "import numpy as np \n",
    "import copy \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters And constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes= 5000\n",
    "max_steps_in_episode=1000\n",
    "learning_rate= 1e-3\n",
    "gamma=0.995\n",
    "max_replays=200000\n",
    "batch_size=64\n",
    "train_interval=3\n",
    "tau= 1e-3 # used for soft update of target network after training. takes a fraction of the model's trained weights\n",
    "\n",
    "layers=[64,64]\n",
    "MSE = torch.nn.MSELoss()\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "memories=ReplayBuffer(max_replays, batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\",new_step_api=True)\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "Actions=np.arange(num_actions)\n",
    "print(f'num_actions={num_actions}\\n state_size={state_size} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instantiating the deep Q networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=utils.DQN(state_size[0],  num_actions, layers)\n",
    "target=copy.deepcopy(model) # target is a clone of the Q network \n",
    "opt=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "target.eval() # target is not trained. Uses soft update to update its weights instead\n",
    "model.train()\n",
    "target.to(device)\n",
    "model.to(device)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs exploitation\n",
    "### When training starts, exploration is favoured over exploitation. $\\epsilon$ decays over training session to favour exploitation over exploration as model learns to take the right steps for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1.0 # starting epsilon\n",
    "decay=0.995 # decay factor per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(dqnModel, epsilon, curState):\n",
    "    model.eval()\n",
    "    action=-1    \n",
    "    if np.random.uniform(0,1) < max(0.05, epsilon) :\n",
    "        action=np.random.choice(Actions) # randomly pick an action       \n",
    "    else:\n",
    "        action=utils.getQAction(dqnModel, state,device)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Q network\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y_j =\n",
    "    \\begin{cases}\n",
    "      R_j & \\text{if episode terminates at step  } j+1\\\\\n",
    "      R_j + \\gamma \\max_{a'}\\hat{Q}(s_{j+1},a') & \\text{otherwise}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDQN():\n",
    "    model.train()\n",
    "    states, actions, rewards, next_states, dones=memories.sample()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_qsa=target(next_states)\n",
    "\n",
    "    max_qsa, _=torch.max(max_qsa, dim=1)\n",
    "    y_targets=rewards + gamma* max_qsa * (1. - dones)\n",
    "\n",
    "    qsa=model(states)    \n",
    "    vals=qsa.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    opt.zero_grad()\n",
    "    loss=MSE(y_targets, vals)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # soft update the target network \n",
    "    utils.softupdate(target, model, tau)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start collecting experiences and train the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AverageRewards=[]\n",
    "Rewards=0.\n",
    "showRes=100\n",
    "totalRewards=0. \n",
    "cnt=0\n",
    "\n",
    "for ep in range (1, (max_episodes+1),1):\n",
    "    state=env.reset()\n",
    "    Rewards=0.\n",
    "    epsilon*=decay\n",
    "    for step in range(1, (max_steps_in_episode+1), 1):        \n",
    "        cnt+=1\n",
    "        action=getAction(model, epsilon, state)\n",
    "        next, reward, done, _,_ = env.step(action)        \n",
    "        memories.add(state, action, reward,next, done)\n",
    "        Rewards+=reward\n",
    "        if len(memories)> batch_size and cnt % train_interval==0:            \n",
    "            trainDQN()\n",
    "        state=next \n",
    "        if done or step ==max_steps_in_episode:\n",
    "            break        \n",
    "    print(f\"\\r episode: {ep} \\t reward: {Rewards}\", end=\"\")\n",
    "    totalRewards+=Rewards\n",
    "    if ep % showRes==0:                  \n",
    "        AverageRewards.append(totalRewards/showRes)     \n",
    "        print(f'\\r Episode:{ep} \\t Average rewards= {AverageRewards[-1] } ') \n",
    "        totalRewards=0.  \n",
    "        if AverageRewards[-1] > 500:\n",
    "            print('\\n\\n training ends early ')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(AverageRewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.runDQNAgent(model, env, device, fps=50)\n",
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.saveTrainedModel(model, 'cartpole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load trained model to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel= utils.DQN(state_size[0],  num_actions, layers)\n",
    "utils.loadModel(trainedModel, \"weights/cartpole\")\n",
    "print(trainedModel)\n",
    "trainedModel.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  run the trained agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.runDQNAgent(trainedModel, env, device, fps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
