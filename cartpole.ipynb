{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole solved with DQN using PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install gym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils \n",
    "from replaybuffer import ReplayBuffer\n",
    "import torch \n",
    "import gymnasium as gym\n",
    "import numpy as np \n",
    "import copy \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters And constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes= 5000\n",
    "max_steps_in_episode=1000\n",
    "learning_rate= 1e-3\n",
    "gamma=0.995\n",
    "max_replays=200000\n",
    "batch_size=64\n",
    "train_interval=3\n",
    "tau= 1e-3 # used for soft update of target network after training. takes a fraction of the model's trained weights\n",
    "\n",
    "layers=[64,64]\n",
    "MSE = torch.nn.MSELoss()\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "memories=ReplayBuffer(max_replays, batch_size, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_actions=2\n",
      " state_size=(4,) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kailo\\miniconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "Actions=np.arange(num_actions)\n",
    "print(f'num_actions={num_actions}\\n state_size={state_size} ')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state=[ 0.0447919   0.00592917 -0.03987714  0.03600601], next=[ 0.04491048 -0.18859892 -0.03915701  0.3158454 ], reward=1.0, done=False \n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "next, reward, done, _,_  = env.step(0)\n",
    "print(f'state={state[0] }, next={next}, reward={reward}, done={done} ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instantiating the deep Q networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=utils.DQN(state_size[0],  num_actions, layers)\n",
    "target=copy.deepcopy(model) # target is a clone of the Q network \n",
    "opt=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "target.eval() # target is not trained. Uses soft update to update its weights instead\n",
    "model.train()\n",
    "target.to(device)\n",
    "model.to(device)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs exploitation\n",
    "### When training starts, exploration is favoured over exploitation. $\\epsilon$ decays over training session to favour exploitation over exploration as model learns to take the right steps for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1.0 # starting epsilon\n",
    "decay=0.995 # decay factor per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(dqnModel, epsilon, curState):\n",
    "    model.eval()\n",
    "    action=-1    \n",
    "    if np.random.uniform(0,1) < max(0.05, epsilon) :\n",
    "        action=np.random.choice(Actions) # randomly pick an action       \n",
    "    else:\n",
    "        action=utils.getQAction(dqnModel, state,device)\n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Q network\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y_j =\n",
    "    \\begin{cases}\n",
    "      R_j & \\text{if episode terminates at step  } j+1\\\\\n",
    "      R_j + \\gamma \\max_{a'}\\hat{Q}(s_{j+1},a') & \\text{otherwise}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDQN():\n",
    "    model.train()\n",
    "    states, actions, rewards, next_states, dones = memories.sample()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_qsa=target(next_states)\n",
    "\n",
    "    max_qsa, _=torch.max(max_qsa, dim=1)\n",
    "    y_targets=rewards + gamma* max_qsa * (1. - dones)\n",
    "\n",
    "    qsa=model(states)    \n",
    "    vals=qsa.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    opt.zero_grad()\n",
    "    loss=MSE(y_targets, vals)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # soft update the target network \n",
    "    utils.softupdate(target, model, tau)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start collecting experiences and train the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode:100 \t Average rewards= 18.03 \n",
      " Episode:200 \t Average rewards= 12.48 \n",
      " Episode:300 \t Average rewards= 10.77 \n",
      " Episode:400 \t Average rewards= 10.06 \n",
      " Episode:500 \t Average rewards= 10.8 \n",
      " Episode:600 \t Average rewards= 14.16 \n",
      " Episode:700 \t Average rewards= 63.7 \n",
      " Episode:800 \t Average rewards= 246.19 \n",
      " Episode:900 \t Average rewards= 210.63 \n",
      " Episode:1000 \t Average rewards= 197.32 \n",
      " Episode:1100 \t Average rewards= 153.05 \n",
      " Episode:1200 \t Average rewards= 164.51 \n",
      " Episode:1300 \t Average rewards= 160.27 \n",
      " Episode:1400 \t Average rewards= 166.66 \n",
      " Episode:1500 \t Average rewards= 172.14 \n",
      " Episode:1600 \t Average rewards= 174.15 \n",
      " Episode:1700 \t Average rewards= 192.35 \n",
      " Episode:1800 \t Average rewards= 258.17 \n",
      " Episode:1900 \t Average rewards= 367.29 \n",
      " Episode:2000 \t Average rewards= 800.14 \n",
      "\n",
      "\n",
      " training ends early \n"
     ]
    }
   ],
   "source": [
    "AverageRewards=[]\n",
    "Rewards=0.\n",
    "showRes=100\n",
    "totalRewards=0. \n",
    "cnt=0\n",
    "\n",
    "for ep in range (1, (max_episodes+1),1):\n",
    "    state=env.reset()\n",
    "    Rewards=0.\n",
    "    epsilon*=decay\n",
    "    for step in range(1, (max_steps_in_episode+1), 1):        \n",
    "        cnt+=1        \n",
    "        if step==1:\n",
    "            state=state[0]\n",
    "        action=getAction(model, epsilon, state)\n",
    "        next, reward, done, _,_ = env.step(action)        \n",
    "        # print(f'adding {state} , {action}, {reward},{next}, {done}')\n",
    "        memories.add(state , action, reward,next, done)                                          \n",
    "        \n",
    "        Rewards+=reward\n",
    "        if len(memories)> batch_size and cnt % train_interval==0:            \n",
    "            trainDQN()\n",
    "        state=next \n",
    "        if done or step ==max_steps_in_episode:\n",
    "            break        \n",
    "    print(f\"\\r episode: {ep} \\t reward: {Rewards}\", end=\"\")\n",
    "    totalRewards+=Rewards\n",
    "    if ep % showRes==0:                  \n",
    "        AverageRewards.append(totalRewards/showRes)     \n",
    "        print(f'\\r Episode:{ep} \\t Average rewards= {AverageRewards[-1] } ') \n",
    "        totalRewards=0.  \n",
    "        if AverageRewards[-1] > 500:\n",
    "            print('\\n\\n training ends early ')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.close()\n",
    "plt.plot(AverageRewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we show the env \n",
    "env = gym.make(\"CartPole-v1\", render_mode='human' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rewards gaained: 332.00\n",
      "Terminated with rewards=332.00\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6234/1718361065.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "utils.runDQNAgent(model, env, device, fps=50)\n",
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.saveTrainedModel(model, 'cartpole')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load trained model to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainedModel= utils.DQN(state_size[0],  num_actions, layers)\n",
    "utils.loadModel(trainedModel, \"weights/cartpole\")\n",
    "print(trainedModel)\n",
    "trainedModel.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  run the trained agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rewards gaained: 1334.00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6234/2336077513.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPole-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/john/data1/git/dqn2/utils.py\u001b[0m in \u001b[0;36mrunDQNAgent\u001b[0;34m(dqnModel, env, device, max_step, fps)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\r rewards gaained: {rewards:.2f}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mstep\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbreakout\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='human' )\n",
    "utils.runDQNAgent(trainedModel, env, device, fps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
